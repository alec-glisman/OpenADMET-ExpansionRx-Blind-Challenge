# Example XGBoost config for per-endpoint training
data:
  endpoints:
    - "LogD"
    - "KSOL"
    - "HLM CLint"
    - "MLM CLint"
    - "Caco-2 Permeability Papp A>B"
    - "Caco-2 Permeability Efflux"
    - "MPPB"
    - "MBPB"
    - "MGMB"

models:
  xgboost:
    early_stopping_rounds: 50
    model_params:
      n_estimators: 500
      learning_rate: 0.05
      max_depth: 6
      subsample: 0.8
      colsample_bytree: 0.8
      reg_alpha: 0.0
      reg_lambda: 1.0

training:
  sample_weights:
    enabled: false
    weights:
      default: 1.0
