# =============================================================================
# Chemprop HPO Configuration Example
# =============================================================================
# This configuration file demonstrates hyperparameter optimization (HPO)
# for Chemprop models using Ray Tune with ASHA scheduler.
#
# Usage:
#   python -m admet.model.chemprop.hpo --config configs/hpo_chemprop.yaml
#
# Or using the bash script:
#   ./scripts/run_chemprop_hpo.sh configs/hpo_chemprop.yaml
# =============================================================================

# Experiment identification
experiment_name: chemprop_hpo_example

# Data configuration
data_path: assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data/split_0/fold_0/train.csv
val_data_path: assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data/split_0/fold_0/val.csv
smiles_column: SMILES
target_columns:
  - "LogD"
  - "Log KSOL"
  - "Log HLM CLint"
  - "Log MLM CLint"
  - "Log Caco-2 Permeability Papp A>B"
  - "Log Caco-2 Permeability Efflux"
  - "Log MPPB"
  - "Log MBPB"
  - "Log MGMB"

# Output configuration
output_dir: assets/models/chemprop/hpo/chemprop_example
ray_storage_path: null # Uses output_dir/ray_results if null

# Reproducibility
seed: 42

# =============================================================================
# Search Space Configuration
# =============================================================================
# Define hyperparameter distributions for optimization.
# Supported types: uniform, loguniform, quniform, choice
#
# For conditional parameters (e.g., MoE-specific), use:
#   conditional_on: ffn_type
#   conditional_values: [moe]
# =============================================================================
search_space:
  # Learning rate (log-uniform for wide range)
  learning_rate:
    type: loguniform
    low: 1.0e-5
    high: 1.0e-2

  # Regularization
  weight_decay:
    type: loguniform
    low: 1.0e-7
    high: 1.0e-3

  dropout:
    type: uniform
    low: 0.0
    high: 0.4

  # Message passing architecture
  depth:
    type: choice
    values: [3, 4, 5, 6]

  hidden_dim:
    type: choice
    values: [256, 512, 768, 1024]

  # FFN architecture
  ffn_num_layers:
    type: choice
    values: [1, 2, 3]

  ffn_hidden_dim:
    type: choice
    values: [256, 512, 768, 1024]

  # Training
  batch_size:
    type: choice
    values: [32, 64, 128]

  # FFN type selection
  ffn_type:
    type: choice
    values: [mlp, moe, branched]

  # MoE-specific (only sampled when ffn_type=moe)
  n_experts:
    type: choice
    values: [2, 4, 8]
    conditional_on: ffn_type
    conditional_values: [moe]

  # Branched-specific (only sampled when ffn_type=branched)
  trunk_depth:
    type: choice
    values: [1, 2, 3]
    conditional_on: ffn_type
    conditional_values: [branched]

  trunk_hidden_dim:
    type: choice
    values: [256, 512, 768]
    conditional_on: ffn_type
    conditional_values: [branched]

  # Message aggregation
  aggregation:
    type: choice
    values: [mean, sum, norm]

  # Per-endpoint loss weights (one weight sampled per target column)
  # Each endpoint gets its own weight uniformly sampled from this range
  target_weights:
    type: uniform
    low: 0.05
    high: 50.0

# =============================================================================
# ASHA Scheduler Configuration
# =============================================================================
# ASHA (Asynchronous Successive Halving Algorithm) provides efficient
# early stopping of underperforming trials.
# =============================================================================
asha:
  # Metric to optimize
  metric: val_mae
  mode: min # 'min' for loss metrics, 'max' for accuracy metrics

  # Training epochs
  max_t: 60 # Maximum epochs for full training
  grace_period: 10 # Minimum epochs before early stopping

  # Successive halving parameters
  reduction_factor: 3 # Keep top 1/3 of trials at each rung
  brackets: 1 # Number of brackets (1 is standard ASHA)

# =============================================================================
# Resource Configuration
# =============================================================================
# Configure computational resources for HPO trials.
# =============================================================================
resources:
  # Number of HPO trials to run
  num_samples: 200

  # Resources per trial
  cpus_per_trial: 2
  gpus_per_trial: 0.25 # 4 concurrent trials per GPU

  # Concurrency limit (null = auto based on resources)
  max_concurrent_trials: null

# =============================================================================
# Transfer Learning Configuration
# =============================================================================
# After HPO, use top-k configurations for full ensemble training.
# =============================================================================
transfer_learning:
  # Number of top configurations to use
  top_k: 10

  # Full training parameters
  full_epochs: 50
  ensemble_size: 5

# =============================================================================
# Curriculum Learning Configuration
# =============================================================================
# Quality-aware curriculum learning progressively adjusts sampling from
# high-quality to lower-quality data based on validation loss improvements.
#
# Note: HPO trials typically run for fewer epochs than full training.
# Consider whether curriculum learning benefits short trials.
# =============================================================================
curriculum:
  # Enable/disable curriculum learning during HPO trials
  enabled: false

  # Column name containing quality labels (e.g., "high", "medium", "low")
  quality_col: "Quality"

  # Quality levels ordered from highest to lowest
  qualities:
    - "high"
    - "medium"
    - "low"

  # Epochs without improvement before phase transition
  # Lower patience for shorter HPO trials
  patience: 3

  # Random seed for reproducible sampling
  seed: 42
