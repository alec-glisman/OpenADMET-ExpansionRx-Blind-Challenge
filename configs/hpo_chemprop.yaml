# =============================================================================
# Chemprop HPO Configuration Example
# =============================================================================
# This configuration file demonstrates hyperparameter optimization (HPO)
# for Chemprop models using Ray Tune with ASHA scheduler.
#
# Usage:
#   python -m admet.model.chemprop.hpo --config configs/hpo_chemprop.yaml
#
# Or using the bash script:
#   ./scripts/run_chemprop_hpo.sh configs/hpo_chemprop.yaml
# =============================================================================

# Experiment identification
experiment_name: chemprop_hpo

# Data configuration
data_path: assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data/split_0/fold_0/train.csv
val_data_path: assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data/split_0/fold_0/validation.csv
smiles_column: SMILES
target_columns:
  - "LogD"
  - "Log KSOL"
  - "Log HLM CLint"
  - "Log MLM CLint"
  - "Log Caco-2 Permeability Papp A>B"
  - "Log Caco-2 Permeability Efflux"
  - "Log MPPB"
  - "Log MBPB"
  - "Log MGMB"

# Per-endpoint loss weights (fixed for all trials)
target_weights:
  - 1.0310
  - 1.0000
  - 1.2252
  - 1.1368
  - 2.1227
  - 2.1174
  - 3.7877
  - 5.2722
  - 10.000

# Output configuration
output_dir: assets/models/chemprop/hpo/chemprop_example
ray_storage_path: /media/aglisman/Data/ray-tune/chemprop_hpo

# Reproducibility
seed: 12345

# =============================================================================
# Search Space Configuration
# =============================================================================
# Define hyperparameter distributions for optimization.
# Supported types: uniform, loguniform, quniform, choice
#
# For conditional parameters (e.g., MoE-specific), use:
#   conditional_on: ffn_type
#   conditional_values: [moe]
# =============================================================================
search_space:
  # Learning rate (log-uniform for wide range)
  learning_rate:
    type: loguniform
    low: 1.0e-5
    high: 1.0e-2

  # Learning rate schedule parameters
  # init_lr = max_lr * lr_warmup_ratio (warmup start)
  lr_warmup_ratio:
    type: uniform
    low: 1.0e-2
    high: 1.0e+2

  # final_lr = max_lr * lr_final_ratio (end of training)
  lr_final_ratio:
    type: uniform
    low: 1.0e-2
    high: 2.0e-1

  # Regularization
  # Note: weight_decay removed - not currently applied in ChempropHyperparams

  dropout:
    type: uniform
    low: 0.0
    high: 0.3

  # Message passing architecture
  depth:
    type: choice
    values: [3, 4, 5, 6]

  # Message hidden dim (MPNN layer width) - can be different from FFN hidden_dim
  message_hidden_dim:
    type: choice
    values: [300, 500, 700, 900, 1100]

  # Legacy hidden_dim (fallback if message_hidden_dim not specified)
  hidden_dim:
    type: choice
    values: [300, 500, 700, 900, 1100]

  # FFN architecture
  ffn_num_layers:
    type: choice
    values: [1, 2, 3, 4]

  ffn_hidden_dim:
    type: choice
    values: [200, 300, 500, 600, 800, 1200]

  # Training
  batch_size:
    type: choice
    values: [32]

  # FFN type selection
  ffn_type:
    type: choice
    values: [mlp, moe, branched]

  # MoE-specific (only sampled when ffn_type=moe)
  n_experts:
    type: choice
    values: [2, 4, 8]
    conditional_on: ffn_type
    conditional_values: [moe]

  # Branched-specific (only sampled when ffn_type=branched)
  trunk_depth:
    type: choice
    values: [1, 2, 3]
    conditional_on: ffn_type
    conditional_values: [branched]

  trunk_hidden_dim:
    type: choice
    values: [300, 400, 500, 600, 800, 1200]
    conditional_on: ffn_type
    conditional_values: [branched]

  # Message aggregation
  aggregation:
    type: choice
    values: [mean, sum, norm]

  # Per-endpoint loss weights (one weight sampled per target column)
  # Each endpoint gets its own weight uniformly sampled from this range
  # Per-endpoint loss weights (one weight sampled per target column)
  # Each endpoint gets its own weight uniformly sampled from this range
  # target_weights:
  #   type: uniform
  #   low: 0.05
  #   high: 10.0
  # Message aggregation
  aggregation:
    type: choice
    values: [mean, sum, norm]

  # Per-endpoint loss weights (one weight sampled per target column)
  # Each endpoint gets its own weight uniformly sampled from this range
  # target_weights:
  #   type: choice
  #   values: [[1.0310, 1.0000, 1.2252, 1.1368, 2.1227, 2.1174, 3.7877, 5.2722, 31.6822]]

# =============================================================================
# ASHA Scheduler Configuration

# =============================================================================
# ASHA Scheduler Configuration
# =============================================================================
# ASHA (Asynchronous Successive Halving Algorithm) provides efficient
# early stopping of underperforming trials.
# =============================================================================
asha:
  # Metric to optimize
  metric: val_mae
  mode: min # 'min' for loss metrics, 'max' for accuracy metrics

  # Training epochs
  max_t: 120 # Maximum epochs for full training
  grace_period: 15 # Minimum epochs before early stopping

  # Successive halving parameters
  reduction_factor: 2 # Keep top 1/2 of trials at each rung
  brackets: 1 # Number of brackets (1 is standard ASHA)

# =============================================================================
# Resource Configuration
# =============================================================================
# Configure computational resources for HPO trials.
# =============================================================================
resources:
  # Number of HPO trials to run
  num_samples: 1000

  # Resources per trial
  cpus_per_trial: 4
  gpus_per_trial: 0.25 # 4 concurrent trials per GPU

  # Concurrency limit (null = auto based on resources)
  max_concurrent_trials: null

# =============================================================================
# Transfer Learning Configuration
# =============================================================================
# After HPO, use top-k configurations for full ensemble training.
# =============================================================================
transfer_learning:
  # Number of top configurations to use
  top_k: 100

  # Full training parameters
  full_epochs: 50
  ensemble_size: 5

# =============================================================================
# Curriculum Learning Configuration
# =============================================================================
# Quality-aware curriculum learning progressively adjusts sampling from
# high-quality to lower-quality data based on validation loss improvements.
#
# Note: HPO trials typically run for fewer epochs than full training.
# Consider whether curriculum learning benefits short trials.
# =============================================================================
curriculum:
  # Enable/disable curriculum learning during HPO trials
  enabled: false

  # Column name containing quality labels (e.g., "high", "medium", "low")
  quality_col: "Quality"

  # Quality levels ordered from highest to lowest
  qualities:
    - "high"
    - "medium"
    - "low"

  # Epochs without improvement before phase transition
  # Lower patience for shorter HPO trials
  patience: 5

  # Random seed for reproducible sampling
  seed: 12345
