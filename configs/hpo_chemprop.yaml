# =============================================================================
# Chemprop HPO Configuration Example
# =============================================================================
# This configuration file demonstrates hyperparameter optimization (HPO)
# for Chemprop models using Ray Tune with ASHA scheduler.
#
# Usage:
#   python -m admet.model.chemprop.hpo --config configs/hpo_chemprop.yaml
#
# Or using the bash script:
#   ./scripts/run_chemprop_hpo.sh configs/hpo_chemprop.yaml
# =============================================================================

# Experiment identification
experiment_name: chemprop-hpo-5
output_dir: /media/aglisman/Data/models/admet-chemprop-hpo-5
ray_storage_path: /media/aglisman/Data/ray-tune-5
mlflow_tracking_uri: "http://127.0.0.1:8080"
# mlflow path: /media/aglisman/Data/models/admet-chemprop

# Data configuration
data_path: assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data/split_0/fold_0/train.csv
val_data_path: assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data/split_0/fold_0/validation.csv
smiles_column: SMILES
target_columns:
  - "LogD"
  - "Log KSOL"
  - "Log HLM CLint"
  - "Log MLM CLint"
  - "Log Caco-2 Permeability Papp A>B"
  - "Log Caco-2 Permeability Efflux"
  - "Log MPPB"
  - "Log MBPB"
  - "Log MGMB"
target_weights:
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0

# Reproducibility
seed: 12345

# =============================================================================
# Search Space Configuration
# =============================================================================
# Define hyperparameter distributions for optimization.
# Supported types: uniform, loguniform, quniform, choice
#
# For conditional parameters (e.g., MoE-specific), use:
#   conditional_on: ffn_type
#   conditional_values: [moe]
# =============================================================================
search_space:
  # max_lr (log-uniform for wide range)
  learning_rate:
    type: loguniform
    low: 1.0e-4
    high: 3.0e-2

  # Learning rate schedule parameters
  # init_lr = max_lr * lr_warmup_ratio (warmup start)
  lr_warmup_ratio:
    type: choice
    values: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]

  # final_lr = max_lr * lr_final_ratio (end of training)
  lr_final_ratio:
    type: choice
    values: [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.08, 0.1, 0.2, 0.5, 1.0]

  dropout:
    type: choice
    values: [0.0, 0.05, 0.1, 0.15, 0.20, 0.25, 0.30]

  # Training
  batch_size:
    type: choice
    values: [32, 64, 128]

  # Task sampling alpha (0=uniform, 1=inverse proportional)
  task_sampling_alpha:
    type: choice
    values: [0.0, 0.02, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9, 1.0]

  # Message passing architecture
  depth:
    type: randint
    low: 2
    high: 8

  # Message hidden dim (MPNN layer width) - can be different from FFN hidden_dim
  message_hidden_dim:
    type: choice
    values: [200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]

  # Message aggregation
  aggregation:
    type: choice
    values: [norm]

  # FFN type selection
  ffn_type:
    type: choice
    values: [mlp, moe, branched]

  # FFN architecture
  ffn_num_layers:
    type: randint
    low: 0
    high: 5

  ffn_hidden_dim:
    type: choice
    values: [200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]

  # MoE-specific (only sampled when ffn_type=moe)
  n_experts:
    type: qrandint
    low: 2
    high: 8
    q: 2
    conditional_on: ffn_type
    conditional_values: [moe]

  # Branched-specific (only sampled when ffn_type=branched)
  trunk_depth:
    type: randint
    low: 0
    high: 5
    conditional_on: ffn_type
    conditional_values: [branched]

  trunk_hidden_dim:
    type: choice
    values: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200]
    conditional_on: ffn_type
    conditional_values: [branched]

# =============================================================================
# ASHA Scheduler Configuration
# =============================================================================
# ASHA (Asynchronous Successive Halving Algorithm) provides efficient
# early stopping of underperforming trials.
# =============================================================================
asha:
  # Metric to optimize
  metric: val_mae
  mode: min # 'min' for loss metrics, 'max' for accuracy metrics

  # Training epochs
  max_t: 120 # Maximum epochs for full training
  grace_period: 15 # Minimum epochs before early stopping

  # Successive halving parameters
  reduction_factor: 3 # Keep top 1/3 of trials at each rung
  brackets: 1 # Number of brackets (1 is standard ASHA)

# =============================================================================
# Resource Configuration
# =============================================================================
# Configure computational resources for HPO trials.
# =============================================================================
resources:
  # Number of HPO trials to run
  num_samples: 2000

  # Resources per trial
  cpus_per_trial: 4
  gpus_per_trial: 0.25 # 4 concurrent trials per GPU

  # Concurrency limit (null = auto based on resources)
  max_concurrent_trials: null

# =============================================================================
# Transfer Learning Configuration
# =============================================================================
# After HPO, use top-k configurations for full ensemble training.
# =============================================================================
transfer_learning:
  # Number of top configurations to use
  top_k: 100

  # Full training parameters
  full_epochs: 60
  ensemble_size: 5

# =============================================================================
# Curriculum Learning Configuration
# =============================================================================
# Quality-aware curriculum learning progressively adjusts sampling from
# high-quality to lower-quality data based on validation loss improvements.
#
# Note: HPO trials typically run for fewer epochs than full training.
# Consider whether curriculum learning benefits short trials.
# =============================================================================
curriculum:
  # Enable/disable curriculum learning during HPO trials
  enabled: false

  # Column name containing quality labels (e.g., "high", "medium", "low")
  quality_col: "Quality"

  # Quality levels ordered from highest to lowest
  qualities:
    - "high"
    - "medium"
    - "low"

  # Epochs without improvement before phase transition
  # Lower patience for shorter HPO trials
  patience: 5

  # Random seed for reproducible sampling
  seed: 12345

# =============================================================================
# Task Affinity Configuration
# =============================================================================
# Compute task affinity matrix during training to identify task relationships
# and group related tasks. The affinity matrix is stored in the trained MPNN
# for downstream use in ensemble models and transfer learning.
#
# Note: Task affinity computation adds overhead at the start of each trial.
# For HPO, consider whether this overhead is acceptable for trial budget.
# =============================================================================
task_affinity:
  # Enable/disable task affinity computation during HPO trials
  enabled: false

  # Number of epochs for affinity estimation
  affinity_epochs: 2

  # Batch size for affinity computation
  affinity_batch_size: 64

  # Learning rate for affinity estimation
  affinity_lr: 0.001

  # Affinity metric: "cosine" or "pearson"
  affinity_type: "cosine"

  # Normalize gradients before computing affinity
  normalize_gradients: true

  # Number of task groups to discover via clustering
  n_groups: 3

  # Clustering algorithm: "agglomerative", "kmeans", or "spectral"
  clustering_method: "agglomerative"

  # Device for computation: "auto", "cpu", or "cuda"
  device: "auto"

  # Random seed for reproducibility
  seed: 42

  # Log affinity matrix and task groups to MLflow
  log_affinity_matrix: true
