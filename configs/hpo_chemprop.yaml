# =============================================================================
# Chemprop HPO Configuration Example
# =============================================================================
# This configuration file demonstrates hyperparameter optimization (HPO)
# for Chemprop models using Ray Tune with ASHA scheduler.
#
# Usage:
#   python -m admet.model.chemprop.hpo --config configs/hpo_chemprop.yaml
#
# Or using the bash script:
#   ./scripts/run_chemprop_hpo.sh configs/hpo_chemprop.yaml
# =============================================================================

# Experiment identification
experiment_name: chemprop-hpo-2

# Data configuration
data_path: assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data/split_0/fold_0/train.csv
val_data_path: assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data/split_0/fold_0/validation.csv
smiles_column: SMILES
target_columns:
  - "LogD"
  - "Log KSOL"
  - "Log HLM CLint"
  - "Log MLM CLint"
  - "Log Caco-2 Permeability Papp A>B"
  - "Log Caco-2 Permeability Efflux"
  - "Log MPPB"
  - "Log MBPB"
  - "Log MGMB"

# Per-endpoint loss weights (fixed for all trials)
target_weights:
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0
  - 1.0

# Output configuration
output_dir: /media/aglisman/Data/models/admet-chemprop-hpo
ray_storage_path: /media/aglisman/Data/ray-tune
mlflow_tracking_uri: "http://127.0.0.1:8080"

# Reproducibility
seed: 12345

# =============================================================================
# Search Space Configuration
# =============================================================================
# Define hyperparameter distributions for optimization.
# Supported types: uniform, loguniform, quniform, choice
#
# For conditional parameters (e.g., MoE-specific), use:
#   conditional_on: ffn_type
#   conditional_values: [moe]
# =============================================================================
search_space:
  # max_lr (log-uniform for wide range)
  learning_rate:
    type: loguniform
    low: 1.0e-4
    high: 1.0e-2

  # Learning rate schedule parameters
  # init_lr = max_lr * lr_warmup_ratio (warmup start)
  lr_warmup_ratio:
    type: choice
    values: [0.1]

  # final_lr = max_lr * lr_final_ratio (end of training)
  lr_final_ratio:
    type: choice
    values: [0.1]

  dropout:
    type: uniform
    low: 0.0
    high: 0.3

  # Message passing architecture
  depth:
    type: randint
    low: 3
    high: 8

  # Message hidden dim (MPNN layer width) - can be different from FFN hidden_dim
  message_hidden_dim:
    type: randint
    low: 300
    high: 1201

  # FFN architecture
  ffn_num_layers:
    type: randint
    low: 1
    high: 5

  ffn_hidden_dim:
    type: randint
    low: 200
    high: 1201

  # Training
  batch_size:
    type: choice
    values: [32]

  # FFN type selection
  ffn_type:
    type: choice
    values: [mlp, moe, branched]

  # MoE-specific (only sampled when ffn_type=moe)
  n_experts:
    type: qrandint
    low: 2
    high: 8
    q: 2
    conditional_on: ffn_type
    conditional_values: [moe]

  # Branched-specific (only sampled when ffn_type=branched)
  trunk_depth:
    type: randint
    low: 1
    high: 5
    conditional_on: ffn_type
    conditional_values: [branched]

  trunk_hidden_dim:
    type: randint
    low: 300
    high: 1201
    conditional_on: ffn_type
    conditional_values: [branched]

  # Message aggregation
  aggregation:
    type: choice
    values: [norm]

  # Task sampling alpha (0=uniform, 1=inverse proportional)
  task_sampling_alpha:
    type: uniform
    low: 0.0
    high: 1.0

# =============================================================================
# ASHA Scheduler Configuration
# =============================================================================
# ASHA (Asynchronous Successive Halving Algorithm) provides efficient
# early stopping of underperforming trials.
# =============================================================================
asha:
  # Metric to optimize
  metric: val_mae
  mode: min # 'min' for loss metrics, 'max' for accuracy metrics

  # Training epochs
  max_t: 100 # Maximum epochs for full training
  grace_period: 15 # Minimum epochs before early stopping

  # Successive halving parameters
  reduction_factor: 3 # Keep top 1/3 of trials at each rung
  brackets: 1 # Number of brackets (1 is standard ASHA)

# =============================================================================
# Resource Configuration
# =============================================================================
# Configure computational resources for HPO trials.
# =============================================================================
resources:
  # Number of HPO trials to run
  num_samples: 2500

  # Resources per trial
  cpus_per_trial: 4
  gpus_per_trial: 0.25 # 4 concurrent trials per GPU

  # Concurrency limit (null = auto based on resources)
  max_concurrent_trials: null

# =============================================================================
# Transfer Learning Configuration
# =============================================================================
# After HPO, use top-k configurations for full ensemble training.
# =============================================================================
transfer_learning:
  # Number of top configurations to use
  top_k: 100

  # Full training parameters
  full_epochs: 60
  ensemble_size: 5

# =============================================================================
# Curriculum Learning Configuration
# =============================================================================
# Quality-aware curriculum learning progressively adjusts sampling from
# high-quality to lower-quality data based on validation loss improvements.
#
# Note: HPO trials typically run for fewer epochs than full training.
# Consider whether curriculum learning benefits short trials.
# =============================================================================
curriculum:
  # Enable/disable curriculum learning during HPO trials
  enabled: false

  # Column name containing quality labels (e.g., "high", "medium", "low")
  quality_col: "Quality"

  # Quality levels ordered from highest to lowest
  qualities:
    - "high"
    - "medium"
    - "low"

  # Epochs without improvement before phase transition
  # Lower patience for shorter HPO trials
  patience: 5

  # Random seed for reproducible sampling
  seed: 12345
