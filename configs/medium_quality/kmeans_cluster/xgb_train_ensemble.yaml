# Example XGBoost config for per-endpoint training
models:
  xgboost:
    # Docs: https://xgboost.readthedocs.io/en/stable/parameter.html
    objective: "mae" # {"mae", "rmse"}
    early_stopping_rounds: null
    model_params:
      device: "cuda"
      n_estimators: 100
      learning_rate: 0.3
      min_split_loss: 0
      max_depth: 6
      subsample: 0.8
      colsample_bytree: 1
      reg_lambda: 1.0
      reg_alpha: 0.0

training:
  output_dir: "assets/models/xgb/ensemble/"
  experiment_name: "xgb_v2_data"
  tracking_uri: null # e.g., "file:///tmp/mlruns" or "http://localhost:5000"
  seed: 123
  sample_weights:
    enabled: false
    weights:
      default: 1.0
  ray:
    multi: true
    address: "local"
    # num_cpus: 8    # limit local Ray runtime to N CPUs (defaults to all)

data:
  root: "assets/dataset/splits/v2/medium_quality/kmeans_cluster"
  endpoints:
    - "LogD"
    - "KSOL"
    - "HLM CLint"
    - "MLM CLint"
    - "Caco-2 Permeability Papp A>B"
    - "Caco-2 Permeability Efflux"
    - "MPPB"
    - "MBPB"
    - "MGMB"
  fingerprint:
    radius: 2
    n_bits: 1024
    use_counts: true
    include_chirality: false
