data:
  data_dir: assets/dataset/split_train_val/v3/quality_high_medium/bitbirch/multilabel_stratified_kfold/data
  test_file: assets/dataset/set/local_test.csv
  blind_file: assets/dataset/set/blind_test.csv
  output_dir: null
  smiles_col: SMILES
  target_cols:
    - LogD
    - Log KSOL
    - Log HLM CLint
    - Log MLM CLint
    - Log Caco-2 Permeability Papp A>B
    - Log Caco-2 Permeability Efflux
    - Log MPPB
    - Log MBPB
    - Log MGMB
  target_weights:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
  splits: null
  folds: null

model:
  depth: 3
  message_hidden_dim: 700
  num_layers: 4
  hidden_dim: 200
  dropout: 0.15
  batch_norm: true
  ffn_type: regression
  aggregation: norm

optimization:
  criterion: MAE
  init_lr: 0.00113
  max_lr: 0.000227
  final_lr: 0.000113
  warmup_epochs: 5
  max_epochs: 150
  patience: 15
  batch_size: 128
  num_workers: 0
  seed: 12345
  progress_bar: false

joint_sampling:
  enabled: true
  num_samples: null
  seed: 42
  increment_seed_per_epoch: true
  log_to_mlflow: true

  task_oversampling:
    alpha: 0.02

  curriculum:
    enabled: true
    quality_col: Quality
    qualities:
      - high
      - medium
      - low
    patience: 5
    strategy: sampled
    reset_early_stopping_on_phase_change: true
    log_per_quality_metrics: true
    seed: 42

    # Count normalization: ensures target proportions are achieved regardless of dataset sizes
    # With imbalanced datasets (e.g., High=5k, Medium=100k, Low=15k), this is essential
    count_normalize: true
    min_high_quality_proportion: 0.25

    # Metric alignment: which metric to monitor for phase transitions and early stopping
    # Options: "val_loss" (default), "val/mae/high", "val/rmse/high", etc.
    monitor_metric: "val/mae/high" # Monitor high-quality validation MAE
    early_stopping_metric: null # If null, uses monitor_metric

    # Adaptive curriculum: auto-adjust proportions based on per-quality performance
    adaptive_enabled: false
    adaptive_improvement_threshold: 0.02 # 2% relative improvement required
    adaptive_max_adjustment: 0.1 # Max 10% adjustment per epoch
    adaptive_lookback_epochs: 5 # Compare current vs N epochs ago

    # Loss weighting: scale gradients by quality level
    loss_weighting_enabled: false
    loss_weights:
      high: 1.0
      medium: 0.5
      low: 0.3

    # HPO-friendly target proportions for each phase
    # These represent the actual fraction of training samples from each quality level
    # Comment out to use defaults: warmup=[0.80, 0.15, 0.05], expand=[0.60, 0.30, 0.10],
    #                              robust=[0.50, 0.35, 0.15], polish=[0.70, 0.20, 0.10]
    # warmup_proportions: [0.80, 0.15, 0.05]
    # expand_proportions: [0.60, 0.30, 0.10]
    # robust_proportions: [0.50, 0.35, 0.15]
    # polish_proportions: [0.70, 0.20, 0.10]

inter_task_affinity:
  enabled: false
  compute_every_n_steps: 2
  log_every_n_steps: 10
  log_epoch_summary: true
  log_step_matrices: false
  lookahead_lr: 0.001
  use_optimizer_lr: true
  exclude_param_patterns:
    - predictor
    - ffn
    - output
    - head
    - readout
  log_to_mlflow: true

ray:
  max_parallel: 1
  num_cpus: null
  num_gpus: null

mlflow:
  tracking: true
  tracking_uri: http://127.0.0.1:8084
  experiment_name: ensemble_chemprop_hpo_topk
  run_name: rank_001
  nested: true
