# Chemprop Configuration with Task Affinity Grouping
# ===================================================
# This configuration enables Task Affinity Grouping (TAG) to automatically
# discover which tasks benefit from joint training and group them accordingly.
#
# Usage:
#   python -m admet.model.chemprop.model --config configs/task-affinity/chemprop_task_affinity.yaml
#
# See docs/guide/task_affinity.rst for detailed documentation.

# Data configuration
data:
  # Directory containing train.csv and validation.csv (required)
  data_dir: "assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data/split_0/fold_0"
  # Optional test and blind files
  test_file: "assets/dataset/set/local_test.csv"
  blind_file: "assets/dataset/set/blind_test.csv"

  # Output directory (null for temp directory)
  output_dir: null

  # Column configuration
  smiles_col: "SMILES"
  target_cols:
    - "LogD"
    - "Log KSOL"
    - "Log HLM CLint"
    - "Log MLM CLint"
    - "Log Caco-2 Permeability Papp A>B"
    - "Log Caco-2 Permeability Efflux"
    - "Log MPPB"
    - "Log MBPB"
    - "Log MGMB"

  # Per-task loss weights (optional, defaults to 1.0 for each)
  target_weights:
    - 1.0310
    - 1.0000
    - 1.2252
    - 1.1368
    - 2.1227
    - 2.1174
    - 3.7877
    - 5.2722
    - 10.000

# Task Affinity Grouping Configuration (Legacy Pre-Training Approach)
# --------------------------------------------------------------------
# DEPRECATED: This uses gradient cosine similarity during a separate
# pre-training phase. For paper-accurate implementation, use
# inter_task_affinity below instead.
#
# Automatically groups tasks based on gradient-based affinity scores
task_affinity:
  # Enable task affinity grouping
  enabled: true

  # Number of task groups to create
  # Recommendation: Start with 3 for 8-10 tasks
  n_groups: 3

  # Affinity computation settings
  affinity_epochs: 1 # Short training run to compute affinities
  affinity_batch_size: 64
  affinity_lr: 0.001

  # Affinity scoring method
  affinity_type: "cosine" # "cosine" or "dot_product"
  normalize_gradients: true

  # Clustering algorithm
  clustering_method: "agglomerative" # "agglomerative" or "spectral"

  # Advanced settings
  encoder_param_patterns: [] # Empty = use default exclusion patterns
  device: "auto" # "auto", "cpu", "cuda", or "cuda:N"
  seed: 42
  log_affinity_matrix: true # Log affinity matrix to console

# =============================================================================
# Inter-Task Affinity Configuration (Paper-Accurate Implementation)
# =============================================================================
# Computes inter-task affinity during training using the lookahead method from:
# "Efficiently Identifying Task Groupings for Multi-Task Learning"
# (Fifty et al., NeurIPS 2021, https://arxiv.org/abs/2109.04617)
#
# Formula: Z^t_{ij} = 1 - L_j(θ^{t+1}_{s|i}) / L_j(θ^t_s)
#
# This measures how task i's gradient update on shared parameters affects
# task j's loss. Positive values indicate beneficial transfer; negative
# values indicate harmful interference.
# =============================================================================
inter_task_affinity:
  # Enable/disable inter-task affinity computation during training
  enabled: false

  # Compute affinity every N training steps (1 = every step)
  compute_every_n_steps: 5

  # Log running average affinity to MLflow every N steps
  log_every_n_steps: 50

  # Log epoch-level summary statistics
  log_epoch_summary: true

  # Log individual step affinity matrices (WARNING: high volume)
  log_step_matrices: false

  # Learning rate for lookahead computation
  lookahead_lr: 0.001

  # Use current optimizer learning rate for lookahead
  use_optimizer_lr: true

  # Patterns to exclude from shared parameters (task-specific layers)
  exclude_param_patterns:
    - predictor
    - ffn
    - output
    - head
    - readout

  # Log affinity metrics to MLflow
  log_to_mlflow: true

# Model architecture configuration
model:
  # Message passing network
  depth: 5
  message_hidden_dim: 600

  # Feed-forward network
  num_layers: 2
  hidden_dim: 600
  dropout: 0.1
  batch_norm: true

  # FFN type: 'regression', 'mixture_of_experts', 'branched'
  ffn_type: "regression"

  # Branched FFN settings (used when ffn_type='branched')
  trunk_n_layers: 2
  trunk_hidden_dim: 600

  # Mixture of Experts settings (used when ffn_type='mixture_of_experts')
  n_experts: 4

# Optimization configuration
optimization:
  criterion: "MSE"
  epochs: 100
  batch_size: 64
  learning_rate: 0.001

  # Learning rate scheduler
  scheduler: "cosine" # "constant", "cosine", "exponential", "step"
  warmup_epochs: 5
  max_lr: 0.001
  final_lr: 0.0001

  # Optimizer settings
  optimizer: "Adam"
  weight_decay: 0.0
  gradient_clip: null

  # Early stopping
  patience: 20
  min_delta: 0.0001

# MLflow tracking
mlflow:
  experiment_name: "chemprop_task_affinity"
  run_name: null # Auto-generated if null
  tracking_uri: "mlruns"
  log_models: true
  log_plots: true

# Reproducibility
seed: 42

# Device configuration
device: "auto" # "auto", "cpu", "cuda", or "cuda:N"

# Logging
log_level: "INFO" # "DEBUG", "INFO", "WARNING", "ERROR"
log_frequency: 10 # Log every N batches
