# Ensemble Curriculum Learning Configuration
# ============================================
# This YAML file configures ensemble training with quality-aware curriculum
# learning across multiple splits and folds. The script will automatically
# discover split_*/fold_*/ subdirectories and train a model on each, using
# curriculum learning to progressively adjust sampling from high-quality
# to lower-quality data.
#
# Curriculum Phases:
#   1. warmup: Focus on high-quality data (e.g., 90% high, 10% medium, 0% low)
#   2. expand: Gradually include medium-quality data (e.g., 60% high, 35% medium, 5% low)
#   3. robust: Include low-quality data for robustness (e.g., 40% high, 40% medium, 20% low)
#   4. polish: Return focus to high-quality data (e.g., 100% high)
#
# Phase transitions occur when overall validation loss stops improving for
# `patience` epochs.
#
# Usage:
#   python -m admet.model.chemprop.ensemble --config configs/ensemble_curriculum.yaml
#   python -m admet.model.chemprop.ensemble -c configs/ensemble_curriculum.yaml --max-parallel 2

# Data configuration for ensemble with curriculum learning
data:
  # Root directory containing split_*/fold_*/ subdirectories
  # Expected structure:
  #   data_dir/
  #     split_0/
  #       fold_0/
  #         train.csv      # Must include Quality column
  #         validation.csv # Must include Quality column
  #       fold_1/
  #         ...
  #     split_1/
  #       ...
  #
  # NOTE: Use all_quality dataset which includes high/medium/low quality labels
  data_dir: "assets/dataset/split_train_val/v3/all_quality/bitbirch/multilabel_stratified_kfold/data"

  # Global test and blind files (same for all ensemble members)
  test_file: "assets/dataset/set/local_test.csv"
  blind_file: "assets/dataset/set/blind_test.csv"

  # Output directory for ensemble results (null for temp directory)
  output_dir: null

  # Column configuration
  smiles_col: "SMILES"
  target_cols:
    - "LogD"
    - "Log KSOL"
    - "Log HLM CLint"
    - "Log MLM CLint"
    - "Log Caco-2 Permeability Papp A>B"
    - "Log Caco-2 Permeability Efflux"
    - "Log MPPB"
    - "Log MBPB"
    - "Log MGMB"

  # Per-task loss weights (optional, defaults to 1.0 for each)
  # Higher weights for tasks with fewer samples or higher importance
  target_weights:
    - 0.5 # LogD - well represented
    - 2.0 # Log KSOL
    - 3.0 # Log HLM CLint
    - 3.0 # Log MLM CLint
    - 5.0 # Log Caco-2 Permeability Papp A>B - sparse
    - 5.0 # Log Caco-2 Permeability Efflux - sparse
    - 2.0 # Log MPPB
    - 3.0 # Log MBPB
    - 3.5 # Log MGMB

  # Optional: filter specific splits/folds (null = use all found)
  # splits: [0, 1, 2]  # Only use splits 0, 1, 2
  # folds: [0, 1, 2, 3, 4]  # Only use folds 0-4
  splits: null
  folds: null

# Model architecture configuration (shared across ensemble)
model:
  depth: 5
  message_hidden_dim: 600
  num_layers: 2
  hidden_dim: 600
  dropout: 0.1
  batch_norm: true
  ffn_type: "regression"
  trunk_n_layers: 2
  trunk_hidden_dim: 600
  n_experts: 4

# Optimization configuration (shared across ensemble)
optimization:
  criterion: "MSE"
  init_lr: 1.0e-4
  max_lr: 1.0e-3
  final_lr: 1.0e-4
  warmup_epochs: 5
  max_epochs: 150
  patience: 15
  batch_size: 32
  num_workers: 0
  seed: 12345
  progress_bar: false # Disable for ensemble (too noisy with parallel training)

# MLflow experiment tracking configuration
mlflow:
  tracking: true
  tracking_uri: "http://127.0.0.1:8080"
  experiment_name: "ensemble_curriculum"
  run_name: null # Auto-generated parent run name
  nested: true # Enable nested runs for ensemble members

# Ensemble-specific settings
# Maximum models to train in parallel (adjust based on GPU memory)
# Each model will get 1/max_parallel of the GPU
max_parallel: 5

# Ray cluster configuration (optional)
# ray_num_cpus: 8  # Limit CPUs for Ray
# ray_num_gpus: 1  # Number of GPUs available
ray_num_cpus: null
ray_num_gpus: null

# Curriculum learning configuration
# =================================
# Enable quality-aware curriculum learning to progressively train on
# data from high to low quality levels. Each ensemble member follows
# its own curriculum progression independently.
curriculum:
  # Enable/disable curriculum learning
  enabled: true

  # Column in train/validation CSV containing quality labels
  # Values should match the qualities list below
  quality_col: "Quality"

  # Quality levels ordered from highest to lowest quality
  # The curriculum adapts to the number of qualities provided:
  #   - 1 quality: warmup → polish
  #   - 2 qualities: warmup → expand → polish
  #   - 3+ qualities: warmup → expand → robust → polish
  qualities:
    - "high"
    - "medium"
    - "low"

  # Number of epochs without improvement in overall validation loss
  # before advancing to the next curriculum phase
  patience: 5

  # Random seed for reproducible curriculum sampling
  # Note: Each ensemble member uses seed + split_idx * 1000 + fold_idx for diversity
  seed: 42

  # Curriculum strategy:
  #   - "sampled": Use weighted random sampling (recommended)
  #     The DynamicCurriculumSampler adjusts sampling weights on each epoch
  #     based on the current curriculum phase.
  #   - "weighted": Apply quality-based loss weights (not yet implemented)
  strategy: "sampled"

  # Whether to reset early stopping patience when advancing to a new
  # curriculum phase. This allows the model more time to adapt to
  # the new data distribution after phase transitions.
  # Recommended: true for curriculum learning to give each phase adequate time
  reset_early_stopping_on_phase_change: true

  # Whether to log per-quality validation metrics (e.g., val_loss_high,
  # val_loss_medium) in addition to overall metrics. Useful for monitoring
  # how the model performs on each quality tier.
  log_per_quality_metrics: true
