# Ensemble ChempropModel Configuration (HPO Rank 52)
# =====================================
# Generated from top_k_configs.json rank 52
# Validation MAE: 0.4797791838645935
#
# This YAML file configures ensemble training across multiple splits and folds.
# The script will automatically discover split_*/fold_*/ subdirectories and
# train a model on each, then aggregate predictions with uncertainty estimates.
#
# Usage:
#   python -m admet.model.chemprop.ensemble --config configs/ensemble_chemprop_hpo_052.yaml
#   python -m admet.model.chemprop.ensemble -c configs/ensemble_chemprop_hpo_052.yaml --max-parallel 2

# Data configuration for ensemble
data:
  data_dir: "assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data"
  splits: null
  folds: null

  test_file: "assets/dataset/set/local_test.csv"
  blind_file: "assets/dataset/set/blind_test.csv"
  output_dir: null

  smiles_col: "SMILES"
  target_cols:
    - "LogD"
    - "Log KSOL"
    - "Log HLM CLint"
    - "Log MLM CLint"
    - "Log Caco-2 Permeability Papp A>B"
    - "Log Caco-2 Permeability Efflux"
    - "Log MPPB"
    - "Log MBPB"
    - "Log MGMB"
  target_weights:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0

model:
  depth: 4
  message_hidden_dim: 1100
  aggregation: "norm"
  ffn_type: "branched"
  num_layers: 3
  hidden_dim: 1200
  trunk_n_layers: 0
  trunk_hidden_dim: 1200
  dropout: 0.05
  batch_norm: true

optimization:
  criterion: "MAE"
  init_lr: 8.83e-04
  max_lr: 1.77e-04
  final_lr: 3.53e-05
  warmup_epochs: 5
  max_epochs: 150
  patience: 15
  batch_size: 64
  num_workers: 0
  seed: 12345
  progress_bar: false
  task_sampling_alpha: 0.05

mlflow:
  tracking: true
  tracking_uri: "http://127.0.0.1:8084"
  experiment_name: "ensemble_chemprop_hpo_topk"
  run_name: "rank_052"
  nested: true

max_parallel: 1
ray_num_cpus: null
ray_num_gpus: null

# Curriculum learning configuration
# =================================
# Quality-aware curriculum learning is disabled by default.
# Enable it when training on mixed-quality datasets.
# For a full curriculum learning ensemble config, see ensemble_curriculum.yaml.
curriculum:
  enabled: false
  quality_col: "Quality"
  qualities:
    - "high"
    - "medium"
    - "low"
  patience: 5
  seed: 42
  # Curriculum strategy: "sampled" (weighted random sampling) or "weighted" (loss weights)
  strategy: "sampled"
  # Reset early stopping when advancing curriculum phases
  reset_early_stopping_on_phase_change: false
  # Log per-quality validation metrics (val_loss_high, val_loss_medium, etc.)
  log_per_quality_metrics: true

# =============================================================================
# Inter-Task Affinity Configuration (Paper-Accurate Implementation)
# =============================================================================
# Computes inter-task affinity during training using the lookahead method from:
# "Efficiently Identifying Task Groupings for Multi-Task Learning"
# (Fifty et al., NeurIPS 2021, https://arxiv.org/abs/2109.04617)
#
# Formula: Z^t_{ij} = 1 - L_j(θ^{t+1}_{s|i}) / L_j(θ^t_s)
# =============================================================================
inter_task_affinity:
  enabled: false
  compute_every_n_steps: 2
  log_every_n_steps: 10
  log_epoch_summary: true
  log_step_matrices: false
  lookahead_lr: 0.001
  use_optimizer_lr: true
  exclude_param_patterns:
    - predictor
    - ffn
    - output
    - head
    - readout
  log_to_mlflow: true
