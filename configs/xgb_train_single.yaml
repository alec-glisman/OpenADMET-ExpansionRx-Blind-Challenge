# Example XGBoost config for per-endpoint training
models:
  xgboost:
    # Docs: https://xgboost.readthedocs.io/en/stable/parameter.html
    objective: "mae" # {"mae", "rmse"}
    early_stopping_rounds: null
    model_params:
      device: "cuda"
      n_estimators: 100 # TODO: HP-tune this
      learning_rate: 0.3 # TODO: HP-tune this
      min_split_loss: 0
      max_depth: 6 # TODO: HP-tune this
      subsample: 0.8 # TODO: HP-tune this
      colsample_bytree: 1
      reg_lambda: 1.0 # TODO: HP-tune this
      reg_alpha: 0.0 # TODO: HP-tune this

training:
  output_dir: "assets/models/xgb/ensemble/high_quality/random_cluster/split_0/fold_0"
  seed: 123
  n_fingerprint_bits: 2048
  sample_weights:
    enabled: false
    weights:
      default: 1.0

ray:
  multi: false
  address: "local"
  # num_cpus: 8    # limit local Ray runtime to N CPUs (defaults to all)

data:
  root: "assets/dataset/splits/high_quality/random_cluster/split_0/fold_0/hf_dataset"
  endpoints:
    - "LogD"
    - "KSOL"
    - "HLM CLint"
    - "MLM CLint"
    - "Caco-2 Permeability Papp A>B"
    - "Caco-2 Permeability Efflux"
    - "MPPB"
    - "MBPB"
    - "MGMB"
