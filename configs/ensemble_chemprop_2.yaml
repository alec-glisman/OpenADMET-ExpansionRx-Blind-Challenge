# Ensemble ChempropModel Configuration
# =====================================
# This YAML file configures ensemble training across multiple splits and folds.
# The script will automatically discover split_*/fold_*/ subdirectories and
# train a model on each, then aggregate predictions with uncertainty estimates.
#
# Usage:
#   python -m admet.model.chemprop.ensemble --config configs/ensemble_chemprop.yaml

# Data configuration for ensemble
data:
  data_dir: "assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data"
  test_file: "assets/dataset/set/local_test.csv"
  blind_file: "assets/dataset/set/blind_test.csv"
  output_dir: null

  # Column configuration
  smiles_col: "SMILES"
  target_cols:
    - "LogD"
    - "Log KSOL"
    - "Log HLM CLint"
    - "Log MLM CLint"
    - "Log Caco-2 Permeability Papp A>B"
    - "Log Caco-2 Permeability Efflux"
    - "Log MPPB"
    - "Log MBPB"
    - "Log MGMB"

  # Per-task loss weights (optional, defaults to 1.0 for each)
  target_weights:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0

  # Optional: filter specific splits/folds (null = use all found)
  # splits: [0, 1, 2]  # Only use splits 0, 1, 2
  # folds: [0, 1, 2, 3, 4]  # Only use folds 0-4
  splits: null
  folds: null

# Model architecture configuration (shared across ensemble)
model:
  depth: 4
  message_hidden_dim: 1100
  aggregation: "norm"
  ffn_type: "regression"
  num_layers: 2
  hidden_dim: 500
  # trunk_n_layers: null
  # trunk_hidden_dim: null
  # n_experts: null
  dropout: 0.25
  batch_norm: false

# Optimization configuration (shared across ensemble)
optimization:
  criterion: "MAE"
  init_lr: 1.04e-5
  max_lr: 1.04e-4
  final_lr: 2.09e-6
  warmup_epochs: 5
  max_epochs: 150
  patience: 15
  batch_size: 128
  num_workers: 0
  seed: 12345
  progress_bar: false
  task_sampling_alpha: 0.1

# MLflow experiment tracking configuration
mlflow:
  tracking: true
  tracking_uri: "http://127.0.0.1:8080"
  experiment_name: "ensemble_chemprop"
  run_name: null # Auto-generated parent run name
  nested: true

# Ensemble-specific settings
# Maximum models to train in parallel (adjust based on GPU memory)
# Each model will get 1/max_parallel of the GPU
max_parallel: 5

# Ray cluster configuration (optional)
# ray_num_cpus: 8  # Limit CPUs for Ray
# ray_num_gpus: 1  # Number of GPUs available
ray_num_cpus: null
ray_num_gpus: null

# Curriculum learning configuration
# =================================
# Quality-aware curriculum learning is disabled by default.
# Enable it when training on mixed-quality datasets.
# For a full curriculum learning ensemble config, see ensemble_curriculum.yaml.
curriculum:
  enabled: false
  quality_col: "Quality"
  qualities:
    - "high"
    - "medium"
    - "low"
  patience: 5
  seed: 42
  # Curriculum strategy: "sampled" (weighted random sampling) or "weighted" (loss weights)
  strategy: "sampled"
  # Reset early stopping when advancing curriculum phases
  reset_early_stopping_on_phase_change: false
  # Log per-quality validation metrics (val_loss_high, val_loss_medium, etc.)
  log_per_quality_metrics: true
