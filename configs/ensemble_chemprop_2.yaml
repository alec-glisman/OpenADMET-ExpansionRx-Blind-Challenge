# Ensemble ChempropModel Configuration
# =====================================
# This YAML file configures ensemble training across multiple splits and folds.
# The script will automatically discover split_*/fold_*/ subdirectories and
# train a model on each, then aggregate predictions with uncertainty estimates.
#
# Usage:
#   python -m admet.model.chemprop.ensemble --config configs/ensemble_chemprop.yaml

# Data configuration for ensemble
data:
  data_dir: "assets/dataset/split_train_val/v3/quality_high/bitbirch/multilabel_stratified_kfold/data"
  test_file: "assets/dataset/set/local_test.csv"
  blind_file: "assets/dataset/set/blind_test.csv"
  output_dir: null

  # Column configuration
  smiles_col: "SMILES"
  target_cols:
    - "LogD"
    - "Log KSOL"
    - "Log HLM CLint"
    - "Log MLM CLint"
    - "Log Caco-2 Permeability Papp A>B"
    - "Log Caco-2 Permeability Efflux"
    - "Log MPPB"
    - "Log MBPB"
    - "Log MGMB"

  # Per-task loss weights (optional, defaults to 1.0 for each)
  target_weights:
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0
    - 1.0

  # Optional: filter specific splits/folds (null = use all found)
  # splits: [0, 1, 2]  # Only use splits 0, 1, 2
  # folds: [0, 1, 2, 3, 4]  # Only use folds 0-4
  splits: null
  folds: null

# Model architecture configuration (shared across ensemble)
model:
  depth: 4
  message_hidden_dim: 1100
  aggregation: "norm"
  ffn_type: "regression"
  num_layers: 2
  hidden_dim: 500
  # trunk_n_layers: null
  # trunk_hidden_dim: null
  # n_experts: null
  dropout: 0.25
  batch_norm: false

# Optimization configuration (shared across ensemble)
optimization:
  criterion: "MAE"
  init_lr: 1.04e-5
  max_lr: 1.04e-4
  final_lr: 2.09e-6
  warmup_epochs: 5
  max_epochs: 150
  patience: 15
  batch_size: 128
  num_workers: 0
  seed: 12345
  progress_bar: false
  task_sampling_alpha: 0.1

# MLflow experiment tracking configuration
mlflow:
  tracking: true
  tracking_uri: "http://127.0.0.1:8084"
  experiment_name: "ensemble_chemprop" # mlflow path: /media/aglisman/Data/models/admet-chemprop
  run_name: null # Auto-generated parent run name
  nested: true

# Ensemble-specific settings
# Maximum models to train in parallel (adjust based on GPU memory)
# Each model will get 1/max_parallel of the GPU
max_parallel: 5

# Ray cluster configuration (optional)
# ray_num_cpus: 8  # Limit CPUs for Ray
# ray_num_gpus: 1  # Number of GPUs available
ray_num_cpus: null
ray_num_gpus: null

# Curriculum learning configuration
# =================================
# Quality-aware curriculum learning is disabled by default.
# Enable it when training on mixed-quality datasets.
# For a full curriculum learning ensemble config, see ensemble_curriculum.yaml.
curriculum:
  enabled: false
  quality_col: "Quality"
  qualities:
    - "high"
    - "medium"
    - "low"
  patience: 5
  seed: 42
  # Curriculum strategy: "sampled" (weighted random sampling) or "weighted" (loss weights)
  strategy: "sampled"
  # Reset early stopping when advancing curriculum phases
  reset_early_stopping_on_phase_change: false
  # Log per-quality validation metrics (val_loss_high, val_loss_medium, etc.)
  log_per_quality_metrics: true

# Task Affinity Configuration (Legacy Pre-Training Approach)
# ============================================================
# DEPRECATED: This uses gradient cosine similarity. For paper-accurate
# implementation, use inter_task_affinity below instead.
#
# Compute task affinity matrix during training to identify task relationships.
# The affinity matrix and task groups are logged to MLflow for downstream
# analysis and decision-making about task groupings for specialized models.
task_affinity:
  enabled: false
  affinity_epochs: 2
  affinity_batch_size: 64
  affinity_lr: 0.001
  affinity_type: "cosine"
  n_groups: 3
  clustering_method: "agglomerative"
  seed: 42

# =============================================================================
# Inter-Task Affinity Configuration (Paper-Accurate Implementation)
# =============================================================================
# Computes inter-task affinity during training using the lookahead method from:
# "Efficiently Identifying Task Groupings for Multi-Task Learning"
# (Fifty et al., NeurIPS 2021, https://arxiv.org/abs/2109.04617)
#
# Formula: Z^t_{ij} = 1 - L_j(θ^{t+1}_{s|i}) / L_j(θ^t_s)
# =============================================================================
inter_task_affinity:
  enabled: false
  compute_every_n_steps: 10
  log_every_n_steps: 100
  log_epoch_summary: true
  log_step_matrices: false
  lookahead_lr: 0.001
  use_optimizer_lr: true
  exclude_param_patterns:
    - predictor
    - ffn
    - output
    - head
    - readout
  log_to_mlflow: true
