{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b72a359",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import itertools\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "\n",
    "import useful_rdkit_utils as uru\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use([\"science\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5648f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb53cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beeca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "level = logging.DEBUG\n",
    "logger = logging.getLogger(__name__)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel(level)\n",
    "\n",
    "logger.info(\"Imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9f1d1",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ca05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data input and output directories\n",
    "base_data_dir = Path().cwd().parents[0] / \"assets/dataset/eda/data/set\"\n",
    "output_dir = base_data_dir.parents[2] / \"splits\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_fig_dir = output_dir / f\"figures/{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "output_fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not base_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found at {base_data_dir}\")\n",
    "\n",
    "logger.info(f\"Output directory set to {output_dir}\")\n",
    "logger.info(f\"Input data directory found at {base_data_dir}\")\n",
    "for dataset_dir in base_data_dir.iterdir():\n",
    "    logger.info(f\"Dataset name: {dataset_dir.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input datasets\n",
    "datasets = {\n",
    "    \"high\": pd.read_csv(base_data_dir / \"cleaned_combined_datasets_high_quality.csv\"),\n",
    "    \"medium\": pd.read_csv(\n",
    "        base_data_dir / \"cleaned_combined_datasets_medium_high_quality.csv\", low_memory=False\n",
    "    ),\n",
    "    \"low\": pd.read_csv(\n",
    "        base_data_dir / \"cleaned_combined_datasets_low_medium_high_quality.csv\", low_memory=False\n",
    "    ),\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    logger.info(f\"Dataset: {name}, shape: {df.shape}\")\n",
    "    logger.info(f\"Columns: {df.columns.tolist()}\")\n",
    "    logger.info(f\"Unique Dataset Constituents: {df['Dataset'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb65fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate fingerprints for all molecules in each dataset\n",
    "fpgen = rdFingerprintGenerator.GetMorganGenerator(\n",
    "    radius=3,\n",
    "    countSimulation=False,\n",
    "    includeChirality=False,\n",
    "    fpSize=2048,\n",
    ")\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    logger.info(f\"Calculating fingerprints for dataset: {name}\")\n",
    "    df[\"mol\"] = df[\"SMILES\"].progress_apply(Chem.MolFromSmiles)\n",
    "    df[\"Fingerprint\"] = df[\"mol\"].progress_apply(fpgen.GetCountFingerprintAsNumPy)\n",
    "\n",
    "    df.drop(columns=[\"mol\"], inplace=True)\n",
    "\n",
    "    # put fingerprint column after \"Molecule Name,SMILES,Dataset\"\n",
    "    cols = df.columns.tolist()\n",
    "    cols.insert(3, cols.pop(cols.index(\"Fingerprint\")))\n",
    "    df = df[cols]\n",
    "\n",
    "    # expand fingerprint numpy arrays into separate columns\n",
    "    fp_array = np.vstack(df[\"Fingerprint\"].values)\n",
    "    fp_df = pd.DataFrame(fp_array, columns=[f\"Morgan_FP_{i}\" for i in range(fp_array.shape[1])])\n",
    "    df = pd.concat([df.reset_index(drop=True), fp_df.reset_index(drop=True)], axis=1)\n",
    "    df.drop(columns=[\"Fingerprint\"], inplace=True)\n",
    "    logger.debug(f\"Number of fingerprint columns added: {fp_df.shape[1]}\")\n",
    "\n",
    "    datasets[name] = df\n",
    "    logger.info(f\"Fingerprints calculated for dataset: {name}\")\n",
    "    logger.debug(f\"Dataset {name} columns after fingerprint calculation: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068efe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on high-quality dataset, sort by Molecule Name ascending and split test/train by first 90%/10%\n",
    "percentage_train = 0.9\n",
    "percentage_validation = 0.1\n",
    "\n",
    "high_quality_df = datasets[\"high\"].sort_values(by=\"Molecule Name\").reset_index(drop=True)\n",
    "n_total = high_quality_df.shape[0]\n",
    "n_train = int(n_total * percentage_train)\n",
    "n_test = n_total - n_train\n",
    "\n",
    "test_df = high_quality_df.iloc[n_train:]\n",
    "train_df = high_quality_df.iloc[:n_train]\n",
    "\n",
    "# randomly split train into train/validation sets\n",
    "train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "n_validation = int(train_df.shape[0] * percentage_validation)\n",
    "validation_df = train_df.iloc[:n_validation]\n",
    "train_df = train_df.iloc[n_validation:]\n",
    "\n",
    "\n",
    "logger.info(f\"High-quality dataset total samples: {n_total}\")\n",
    "logger.info(f\"Training samples: {train_df.shape[0]}\")\n",
    "logger.info(f\"Validation samples: {validation_df.shape[0]}\")\n",
    "logger.info(f\"Testing samples: {test_df.shape[0]}\")\n",
    "\n",
    "# save to temporal datasplit\n",
    "temporal_dir = output_dir / \"high_quality/temporal_split\"\n",
    "temporal_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# convert to hf dataset and save\n",
    "train_hf = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "validation_hf = Dataset.from_pandas(validation_df, preserve_index=False)\n",
    "test_hf = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "temporal_hf = DatasetDict({\"train\": train_hf, \"validation\": validation_hf, \"test\": test_hf})\n",
    "# save to disk\n",
    "temporal_hf.save_to_disk(str(temporal_dir))\n",
    "logger.info(f\"Temporal split datasets saved to {temporal_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab8d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "n_splits = 5\n",
    "percentage_validation = 0.1\n",
    "stratify_column = \"Dataset\"\n",
    "\n",
    "split_dict = {\n",
    "    \"random_cluster\": uru.get_random_clusters,\n",
    "    \"scaffold_cluster\": uru.get_bemis_murcko_clusters,\n",
    "    \"kmeans_cluster\": uru.get_kmeans_clusters,  # n_clusters = 10 by default\n",
    "    # \"butina_cluster\": uru.get_butina_clusters,  # cutoff = 0.65 by default\n",
    "    \"umap_cluster\": uru.get_umap_clusters,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cec710",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = {}\n",
    "\n",
    "n_iter = len(datasets) * len(split_dict) * n_splits\n",
    "logger.info(f\"Total iterations for dataset splits: {n_iter}\")\n",
    "\n",
    "pbar = tqdm(total=n_iter, desc=\"Creating dataset splits\")\n",
    "for dset_name, data in datasets.items():  # iterate over different datasets\n",
    "    split_datasets[dset_name] = {}\n",
    "\n",
    "    for split_name, split in split_dict.items():  # iterate over different splitting methods\n",
    "        logger.info(f\"Processing dataset: {dset_name} with split method: {split_name}\")\n",
    "        split_datasets[dset_name][split_name] = {}\n",
    "\n",
    "        for i in range(0, n_splits):  # iterate over different splits\n",
    "            split_datasets[dset_name][split_name][f\"split_{i}\"] = {}\n",
    "            group_kfold_shuffle = uru.GroupKFoldShuffle(n_splits=n_folds, random_state=i, shuffle=True)\n",
    "\n",
    "            for group in data[stratify_column].unique():  # iterate over different dataset groups\n",
    "                # stratified group k-fold split (based on \"Dataset\" column)\n",
    "                subdata = data[data[stratify_column] == group]\n",
    "                cluster_list = split(subdata.SMILES)\n",
    "\n",
    "                # make fictitious subdata indices to map back to original data later\n",
    "                subdata_indices = subdata.index.to_numpy()\n",
    "\n",
    "                # iterate over different folds within each split\n",
    "                for j, (subdata_train_idx, subdata_test_idx) in enumerate(\n",
    "                    group_kfold_shuffle.split(subdata_indices, groups=cluster_list)\n",
    "                ):\n",
    "                    if f\"fold_{j}\" not in split_datasets[dset_name][split_name][f\"split_{i}\"]:\n",
    "                        split_datasets[dset_name][split_name][f\"split_{i}\"][f\"fold_{j}\"] = {}\n",
    "\n",
    "                    # map indices back to original data\n",
    "                    train_idx = subdata_indices[subdata_train_idx]\n",
    "                    test_idx = subdata_indices[subdata_test_idx]\n",
    "\n",
    "                    # further split train_idx into train and validation sets\n",
    "                    n_train_samples = len(train_idx)\n",
    "                    n_val_samples = int(n_train_samples * percentage_validation)\n",
    "                    np.random.seed(i + j)  # ensure reproducibility\n",
    "                    shuffled_train_idx = np.random.permutation(train_idx)\n",
    "                    val_idx = shuffled_train_idx[:n_val_samples]\n",
    "                    train_idx = shuffled_train_idx[n_val_samples:]\n",
    "\n",
    "                    # save indices for each group split\n",
    "                    split_datasets[dset_name][split_name][f\"split_{i}\"][f\"fold_{j}\"][group] = {\n",
    "                        \"train\": train_idx,\n",
    "                        \"validation\": val_idx,\n",
    "                        \"test\": test_idx,\n",
    "                    }\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "            # combine group splits into final train/test sets for each fold\n",
    "            # logger.debug(f\"Combining group splits for dataset: {dset_name}, split: {split_name}, iteration: {i}\")\n",
    "            for j in range(n_folds):\n",
    "                if f\"fold_{j}\" not in split_datasets[dset_name][split_name][f\"split_{i}\"]:\n",
    "                    raise ValueError(\n",
    "                        f\"Fold {j} not found in split {i} for dataset {dset_name} and split method {split_name}\"\n",
    "                    )\n",
    "\n",
    "                combined_train_indices = []\n",
    "                combined_val_indices = []\n",
    "                combined_test_indices = []\n",
    "\n",
    "                for group in data[stratify_column].unique():\n",
    "                    group_split = split_datasets[dset_name][split_name][f\"split_{i}\"][f\"fold_{j}\"][group]\n",
    "                    combined_train_indices.extend(group_split[\"train\"])\n",
    "                    combined_val_indices.extend(group_split[\"validation\"])\n",
    "                    combined_test_indices.extend(group_split[\"test\"])\n",
    "\n",
    "                combined_train_indices = np.array(combined_train_indices)\n",
    "                combined_val_indices = np.array(combined_val_indices)\n",
    "                combined_test_indices = np.array(combined_test_indices)\n",
    "\n",
    "                # save combined train/test sets\n",
    "                split_datasets[dset_name][split_name][f\"split_{i}\"][f\"fold_{j}\"][\"total\"] = {\n",
    "                    \"train\": combined_train_indices,\n",
    "                    \"validation\": combined_val_indices,\n",
    "                    \"test\": combined_test_indices,\n",
    "                }\n",
    "\n",
    "                # final assertions\n",
    "\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80de99b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all datasets with name format {dataset}_quality/{split_method}/split{split_number}_fold{fold_number}.csv\n",
    "for dset_name, splits in split_datasets.items():\n",
    "    for split_name, split_data in splits.items():\n",
    "        for split_number, folds in split_data.items():\n",
    "            for fold_number, datasets_dict in folds.items():\n",
    "\n",
    "                split_output_dir = (\n",
    "                    output_dir / f\"{dset_name}_quality/{split_name}/{split_number}/{fold_number}\"\n",
    "                )\n",
    "                split_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                logger.debug(f\"Saving dataset to {split_output_dir}\")\n",
    "\n",
    "                train_idx = datasets_dict[\"total\"][\"train\"]\n",
    "                val_idx = datasets_dict[\"total\"][\"validation\"]\n",
    "                test_idx = datasets_dict[\"total\"][\"test\"]\n",
    "\n",
    "                # convert pandas to HF dataset\n",
    "                train_hf = Dataset.from_pandas(data.loc[train_idx], preserve_index=False)\n",
    "                val_hf = Dataset.from_pandas(data.loc[val_idx], preserve_index=False)\n",
    "                test_hf = Dataset.from_pandas(data.loc[test_idx], preserve_index=False)\n",
    "                dset = DatasetDict({\"train\": train_hf, \"validation\": val_hf, \"test\": test_hf})\n",
    "\n",
    "                # Save to disk as HF dataset\n",
    "                dset.save_to_disk(f\"{split_output_dir}/hf_dataset\")\n",
    "\n",
    "        # print size of folder in MB after saving all splits\n",
    "        folder_size = sum(f.stat().st_size for f in split_output_dir.glob(\"**/*\") if f.is_file())\n",
    "        folder_size_mb = folder_size / (1024 * 1024)\n",
    "        logger.info(\n",
    "            f\"Saved all splits for dataset: {dset_name}, split method: {split_name}. Folder size: {folder_size_mb:.2f} MB\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the number of samples for each endpoint in each split\n",
    "for dset_name, splits in split_datasets.items():\n",
    "    for split_name, split_data in splits.items():\n",
    "        for split_number, folds in split_data.items():\n",
    "            for fold_number, datasets_dict in folds.items():\n",
    "                split_output_dir = (\n",
    "                    output_dir / f\"{dset_name}_quality/{split_name}/{split_number}/{fold_number}\"\n",
    "                )\n",
    "\n",
    "                train_idx = datasets_dict[\"total\"][\"train\"]\n",
    "                val_idx = datasets_dict[\"total\"][\"validation\"]\n",
    "                test_idx = datasets_dict[\"total\"][\"test\"]\n",
    "\n",
    "                train_df = datasets[dset_name].loc[train_idx]\n",
    "                val_df = datasets[dset_name].loc[val_idx]\n",
    "                test_df = datasets[dset_name].loc[test_idx]\n",
    "\n",
    "                # count number of samples for each endpoint\n",
    "                endpoints = [\n",
    "                    \"LogD\",\n",
    "                    \"KSOL\",\n",
    "                    \"HLM CLint\",\n",
    "                    \"MLM CLint\",\n",
    "                    \"Caco-2 Permeability Papp A>B\",\n",
    "                    \"Caco-2 Permeability Efflux\",\n",
    "                    \"MPPB\",\n",
    "                    \"MBPB\",\n",
    "                    \"MGMB\",\n",
    "                ]\n",
    "\n",
    "                counts = {\n",
    "                    \"train\": [train_df[ep].notnull().sum() for ep in endpoints],\n",
    "                    \"validation\": [val_df[ep].notnull().sum() for ep in endpoints],\n",
    "                    \"test\": [test_df[ep].notnull().sum() for ep in endpoints],\n",
    "                }\n",
    "\n",
    "                counts_df = pd.DataFrame(counts, index=endpoints)\n",
    "\n",
    "                # plot\n",
    "                ax = counts_df.plot.bar(rot=45, figsize=(10, 6))\n",
    "                ax.set_title(\n",
    "                    f\"Dataset: {dset_name}, Split: {split_name}, {split_number}, {fold_number} - Sample Counts per Endpoint\"\n",
    "                )\n",
    "                ax.set_ylabel(\"Number of Samples\")\n",
    "                plt.tight_layout()\n",
    "                plt_path = split_output_dir / \"sample_counts_per_endpoint.png\"\n",
    "                plt.savefig(plt_path, dpi=600)\n",
    "                plt.close()\n",
    "                logger.info(f\"Saved sample counts plot to {plt_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot for number of test samples for each split method (x) on different datasets (separate plots)\n",
    "for dset_name, splits in split_datasets.items():\n",
    "    logger.info(f\"Creating boxplot for dataset: {dset_name}\")\n",
    "\n",
    "    plot_data = []\n",
    "    for split_name, split_data in splits.items():\n",
    "        for split_id, folds in split_data.items():\n",
    "            for fold_id, datasets in folds.items():\n",
    "                n_test_samples = len(datasets[\"total\"][\"test\"])\n",
    "                plot_data.append({\"Split Method\": split_name, \"Number of Test Samples\": n_test_samples})\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.boxplot(x=\"Split Method\", y=\"Number of Test Samples\", data=plot_df, ax=ax)\n",
    "    ax.set_title(f\"Distribution of Test Set Sizes for Different Split Methods\\nDataset: {dset_name}\")\n",
    "    ax.set_ylabel(\"Number of Test Samples\")\n",
    "    ax.set_xlabel(\"Split Method\")\n",
    "    ax.yaxis.get_major_locator().set_params(integer=True)\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(output_fig_dir / f\"{dset_name}_test_set_size_distribution.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot distribution of data points for each split method and each dataset\n",
    "for dset_name, splits in split_datasets.items():\n",
    "    for split_name, split_data in splits.items():\n",
    "        fold_sizes = []\n",
    "        for split_id, folds in split_data.items():\n",
    "            for fold_id, groups in folds.items():\n",
    "                for group_name, datasets in groups.items():\n",
    "                    # ignore \"total\"\n",
    "                    if group_name == \"total\":\n",
    "                        continue\n",
    "\n",
    "                    train_size = len(datasets[\"train\"])\n",
    "                    test_size = len(datasets[\"test\"])\n",
    "                    fold_sizes.append(\n",
    "                        {\n",
    "                            \"Split ID\": split_id,\n",
    "                            \"Fold ID\": fold_id,\n",
    "                            \"Group\": group_name,\n",
    "                            \"Train Size\": train_size,\n",
    "                            \"Test Size\": test_size,\n",
    "                        }\n",
    "                    )\n",
    "        fold_sizes_df = pd.DataFrame(fold_sizes)\n",
    "\n",
    "        # 1 figure with 2 boxplots: train size and test size\n",
    "        logger.info(\n",
    "            f\"Creating train/test size distribution boxplots for dataset: {dset_name}, split method: {split_name}\"\n",
    "        )\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        sns.boxplot(x=\"Group\", y=\"Train Size\", data=fold_sizes_df, ax=axs[0])\n",
    "        axs[0].set_title(\n",
    "            f\"Train Set Size Distribution: {dset_name.capitalize()} Quality, {split_name.replace('_', ' ').capitalize()} Split\"\n",
    "        )\n",
    "\n",
    "        sns.boxplot(x=\"Group\", y=\"Test Size\", data=fold_sizes_df, ax=axs[1])\n",
    "        axs[1].set_title(\n",
    "            f\"Test Set Size Distribution: {dset_name.capitalize()} Quality, {split_name.replace('_', ' ').capitalize()} Split\"\n",
    "        )\n",
    "\n",
    "        for ax in axs:\n",
    "            ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "            ax.tick_params(axis=\"x\", rotation=45)\n",
    "            ax.set_ylabel(\"Number of Data Points\")\n",
    "            ax.set_xlabel(\"Provenance\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(\n",
    "            output_fig_dir\n",
    "            / f\"{dset_name}_quality_{split_name}_split_train_test_size_distribution_boxplot.png\",\n",
    "            dpi=600,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenADMET-ExpansionRx-Blind-Challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
