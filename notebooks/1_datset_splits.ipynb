{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b72a359",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7f1ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "import itertools\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scienceplots\n",
    "\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "import useful_rdkit_utils as uru\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "from rdkit.DataStructs import BulkTanimotoSimilarity\n",
    "\n",
    "from admet.model.lgbm_wrapper import LGBMMorganCountWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcdf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(['science'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5648f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb53cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beeca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup logging\n",
    "level = logging.DEBUG\n",
    "logger = logging.getLogger(__name__)\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "ch = logging.StreamHandler()\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel(level)\n",
    "\n",
    "logger.info(\"Imports successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9f1d1",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ca05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data input and output directories\n",
    "base_data_dir = Path().cwd().parents[0] / \"assets/dataset/eda/data/set\"\n",
    "output_dir = base_data_dir.parents[2] / \"splits\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "output_fig_dir = output_dir / \"figures\"\n",
    "output_fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not base_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Data directory not found at {base_data_dir}\")\n",
    "\n",
    "logger.info(f\"Output directory set to {output_dir}\")\n",
    "logger.info(f\"Input data directory found at {base_data_dir}\")\n",
    "for dataset_dir in base_data_dir.iterdir():\n",
    "    logger.info(f\"Dataset name: {dataset_dir.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad02795",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpgen = rdFingerprintGenerator.GetMorganGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input datasets\n",
    "datasets = {\n",
    "    \"high\": pd.read_csv(base_data_dir / \"cleaned_combined_datasets_high_quality.csv\"),\n",
    "    \"medium\": pd.read_csv(\n",
    "        base_data_dir / \"cleaned_combined_datasets_medium_high_quality.csv\", low_memory=False\n",
    "    ),\n",
    "    \"low\": pd.read_csv(\n",
    "        base_data_dir / \"cleaned_combined_datasets_low_medium_high_quality.csv\", low_memory=False\n",
    "    ),\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    logger.info(f\"Dataset: {name}, shape: {df.shape}\")\n",
    "    logger.info(f\"Columns: {df.columns.tolist()}\")\n",
    "    logger.info(f\"Unique Dataset Constituents: {df['Dataset'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068efe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on high-quality dataset, sort by Molecule Name ascending and split test/train by first 90%/10%\n",
    "percentage_train = 0.9\n",
    "\n",
    "high_quality_df = datasets[\"high\"].sort_values(by=\"Molecule Name\").reset_index(drop=True)\n",
    "n_total = high_quality_df.shape[0]\n",
    "n_train = int(n_total * percentage_train)\n",
    "n_test = n_total - n_train\n",
    "\n",
    "train_df = high_quality_df.iloc[:n_train]\n",
    "test_df = high_quality_df.iloc[n_train:]\n",
    "\n",
    "logger.info(f\"High-quality dataset total samples: {n_total}\")\n",
    "logger.info(f\"Training samples: {train_df.shape[0]}\")\n",
    "logger.info(f\"Testing samples: {test_df.shape[0]}\")\n",
    "\n",
    "# save to temporal datasplit\n",
    "temporal_dir = output_dir / \"high_quality/temporal_split\"\n",
    "temporal_dir.mkdir(parents=True, exist_ok=True)\n",
    "train_df.to_csv(temporal_dir / \"train.csv\", index=False)\n",
    "test_df.to_csv(temporal_dir / \"test.csv\", index=False)\n",
    "logger.info(f\"Temporal split datasets saved to {temporal_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab8d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "n_splits = 5\n",
    "stratify_column = \"Dataset\"\n",
    "\n",
    "split_dict = {\n",
    "    \"random_cluster\": uru.get_random_clusters,\n",
    "    \"scaffold_cluster\": uru.get_bemis_murcko_clusters,\n",
    "    \"kmeans_cluster\": uru.get_kmeans_clusters,  # n_clusters = 10 by default\n",
    "    \"butina_cluster\": uru.get_butina_clusters,  # cutoff = 0.65 by default\n",
    "    # \"umap_cluster\": uru.get_umap_clusters,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cec710",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = {}\n",
    "\n",
    "n_iter = len(datasets) * len(split_dict) * n_splits\n",
    "logger.info(f\"Total iterations for dataset splits: {n_iter}\")\n",
    "\n",
    "pbar = tqdm(total=n_iter, desc=\"Creating dataset splits\")\n",
    "for dset_name, data in datasets.items():  # iterate over different datasets\n",
    "    split_datasets[dset_name] = {}\n",
    "\n",
    "    for split_name, split in split_dict.items():  # iterate over different splitting methods\n",
    "        logger.info(f\"Processing dataset: {dset_name} with split method: {split_name}\")\n",
    "        split_datasets[dset_name][split_name] = {}\n",
    "\n",
    "        for i in range(0, n_splits):  # iterate over different splits\n",
    "            split_datasets[dset_name][split_name][f\"split_{i}\"] = {}\n",
    "            group_kfold_shuffle = uru.GroupKFoldShuffle(n_splits=n_folds, random_state=i, shuffle=True)\n",
    "\n",
    "            for group in data[stratify_column].unique():  # iterate over different dataset groups\n",
    "                 # stratified group k-fold split (based on \"Dataset\" column)\n",
    "                subdata = data[data[stratify_column] == group].reset_index(drop=True)\n",
    "                cluster_list = split(subdata.SMILES)\n",
    "                \n",
    "                # iterate over different folds within each split\n",
    "                for j, (train_idx, test_idx) in enumerate(group_kfold_shuffle.split(subdata, groups=cluster_list)):\n",
    "                    if f\"fold_{j}\" not in split_datasets[dset_name][split_name][f\"split_{i}\"]:\n",
    "                        split_datasets[dset_name][split_name][f\"split_{i}\"][f\"fold_{j}\"] = {}\n",
    "                        \n",
    "                    split_datasets[dset_name][split_name][f\"split_{i}\"][f\"fold_{j}\"][group] = {\n",
    "                        \"train\": subdata.loc[train_idx].reset_index(drop=True).copy(),\n",
    "                        \"test\": subdata.loc[test_idx].reset_index(drop=True).copy(),\n",
    "                    }\n",
    "            \n",
    "            # combine group splits into final train/test sets for each fold\n",
    "            logger.debug(f\"Combining group splits for dataset: {dset_name}, split: {split_name}, iteration: {i}\")\n",
    "            for j in range(n_folds):\n",
    "                combined_train = pd.concat(\n",
    "                    [\n",
    "                        split_datasets[dset_name][split_name][f\"split_{i}\"][f\"fold_{j}\"][group][\"train\"]\n",
    "                        for group in data[stratify_column].unique()\n",
    "                    ],\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "                combined_test = pd.concat(\n",
    "                    [\n",
    "                        split_datasets[dset_name][split_name][f\"split_{i}\"][f\"fold_{j}\"][group][\"test\"]\n",
    "                        for group in data[stratify_column].unique()\n",
    "                    ],\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "                split_datasets[dset_name][split_name][f\"split_{i}\"][f\"fold_{j}\"][\"total\"] = {\n",
    "                    \"train\": combined_train,\n",
    "                    \"test\": combined_test,\n",
    "                }\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d29d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all datasets with name format {dataset}_quality/{split_method}/split{split_number}_fold{fold_number}.csv\n",
    "for dset_name, splits in split_datasets.items():\n",
    "    for split_name, split_data in splits.items():\n",
    "        for split_number, folds in split_data.items():\n",
    "            for fold_number, datasets_dict in folds.items():\n",
    "                train_df = datasets_dict[\"total\"][\"train\"]\n",
    "                test_df = datasets_dict[\"total\"][\"test\"]\n",
    "                \n",
    "                split_output_dir = output_dir / f\"{dset_name}_quality\" / split_name\n",
    "                split_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                train_file = split_output_dir / f\"{split_number}_{fold_number}_train.csv\"\n",
    "                test_file = split_output_dir / f\"{split_number}_{fold_number}_test.csv\"\n",
    "                \n",
    "                train_df.to_csv(train_file, index=False)\n",
    "                test_df.to_csv(test_file, index=False)\n",
    "                \n",
    "                logger.info(f\"Saved train set to {train_file} with shape {train_df.shape}\")\n",
    "                logger.info(f\"Saved test set to {test_file} with shape {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot for number of test samples for each split method (x) on different datasets (separate plots)\n",
    "for dset_name, splits in split_datasets.items():\n",
    "    logger.info(f\"Creating boxplot for dataset: {dset_name}\")\n",
    "\n",
    "    plot_data = []\n",
    "    for split_name, split_data in splits.items():\n",
    "        for split_id, folds in split_data.items():\n",
    "            for fold_id, datasets in folds.items():\n",
    "                n_test_samples = len(datasets[\"total\"][\"test\"])\n",
    "                plot_data.append({\n",
    "                    \"Split Method\": split_name,\n",
    "                    \"Number of Test Samples\": n_test_samples\n",
    "                })\n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.boxplot(x=\"Split Method\", y=\"Number of Test Samples\", data=plot_df, ax=ax)\n",
    "    ax.set_title(f\"Distribution of Test Set Sizes for Different Split Methods\\nDataset: {dset_name}\")\n",
    "    ax.set_ylabel(\"Number of Test Samples\")\n",
    "    ax.set_xlabel(\"Split Method\")\n",
    "    ax.yaxis.get_major_locator().set_params(integer=True)\n",
    "    ax.tick_params(axis=\"x\", rotation=45)\n",
    "    ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(output_fig_dir / f\"{dset_name}_test_set_size_distribution.png\", dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a5f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot distribution of data points for each split method and each dataset\n",
    "\n",
    "for dset_name, splits in split_datasets.items():\n",
    "    for split_name, split_data in splits.items():\n",
    "        fold_sizes = []\n",
    "        for split_id, folds in split_data.items():\n",
    "            for fold_id, groups in folds.items():\n",
    "                for group_name, datasets in groups.items():\n",
    "                    train_size = len(datasets[\"train\"])\n",
    "                    test_size = len(datasets[\"test\"])\n",
    "                    fold_sizes.append(\n",
    "                        {\n",
    "                            \"Split ID\": split_id,\n",
    "                            \"Fold ID\": fold_id,\n",
    "                            \"Group\": group_name,\n",
    "                            \"Train Size\": train_size,\n",
    "                            \"Test Size\": test_size,\n",
    "                        }\n",
    "                    )\n",
    "        fold_sizes_df = pd.DataFrame(fold_sizes)\n",
    "        \n",
    "\n",
    "        # 1 figure with 2 boxplots: train size and test size\n",
    "        logger.info(f\"Creating train/test size distribution boxplots for dataset: {dset_name}, split method: {split_name}\")\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        \n",
    "        sns.boxplot(x=\"Group\", y=\"Train Size\", data=fold_sizes_df, ax=axs[0])\n",
    "        axs[0].set_title(f\"Train Set Size Distribution: {dset_name.capitalize()} Quality, {split_name.replace('_', ' ').capitalize()} Split\")\n",
    "       \n",
    "        sns.boxplot(x=\"Group\", y=\"Test Size\", data=fold_sizes_df, ax=axs[1])\n",
    "        axs[1].set_title(f\"Test Set Size Distribution: {dset_name.capitalize()} Quality, {split_name.replace('_', ' ').capitalize()} Split\")\n",
    "\n",
    "        for ax in axs:\n",
    "            ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "            ax.tick_params(axis='x', rotation=45)\n",
    "            ax.set_ylabel(\"Number of Data Points\")\n",
    "            ax.set_xlabel(\"Provenance\")\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        fig.savefig(\n",
    "            output_fig_dir\n",
    "            / f\"{dset_name}_quality_{split_name}_split_train_test_size_distribution_boxplot.png\",\n",
    "            dpi=600,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenADMET-ExpansionRx-Blind-Challenge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
